# Mirel-Tuner

**Status: Alpha Dry-Run Verified âœ… | Windows-Compatible ðŸªŸ | Under Active Development**

Mirel-Tuner is a modular AI training orchestrator designed for Stable Diffusion XL and beyond. It enables per-layer regulation, dynamic scheduler overrides, and hybrid multiprocessing with per-device model allocation. This repo was collaboratively built by Philip (Captain) and GPT O3, with critical architecture and tuning logic now verified through a successful dry run.

---

## âœ… Dry Run Status

- ðŸ”¹ Core execution path launches cleanly.
- ðŸ”¹ Device targeting (CUDA 0,1) confirmed operational.
- ðŸ”¹ Accelerate-free multiprocessing mode validated.
- ðŸ”¹ Directory structure, config loading, and model init confirmed.

> ðŸ’¡ While not yet training-ready, the dry run confirms baseline system stability.

---


## ðŸ§° Requirements (Windows-Focused)

Weâ€™ve selected a lean but capable requirements set optimized for Windows-based development and GPU acceleration (CUDA 12.1). Core dependencies include:

```
torch==2.1.2+cu121
transformers>=4.36.2
diffusers==0.27.2
safetensors==0.4.2
accelerate==0.25.0
einops
huggingface_hub
xformers==0.0.23.post1
```

Setup Install with the windows setup.bat script:

Open your command prompt to the directory where you cloned the repository and run:
```
setup.bat
```

This will create a virtual environment and install the required packages.

## âš ï¸ Manual Install

Manually Install with:

```
pip install torch==2.1.2+cu121 -f https://download.pytorch.org/whl/torch_stable.html
pip install -r requirements.txt
```

âš ï¸ For CUDA GPU users, ensure your driver supports CUDA 12.1. 

âš ï¸ âš ï¸ âš ï¸ This configuration has been verified on ONLY ONE windows setup using these reqs as of updating this.


---

## ðŸ“‚ Directory Overview

```
mirel-tuner/
â”‚
â”œâ”€â”€ associate/        # Runtime-active model containers, caches, loaders
â”œâ”€â”€ program/          # Core program logic: schedulers, loss modules, engine
â”œâ”€â”€ configs/          # JSON and Python config sets
â”œâ”€â”€ scripts/          # Utility and task scripts (notebook runners, preprocessing)
â”œâ”€â”€ tests/            # Verification stubs for future unit coverage
â””â”€â”€ README.md         # You're here.
```

---

## ðŸš§ In Development

We are actively building:

- Layer and timestep schedulers (`program/schedulers/`)
- Custom noise augmentations and anchor regulation (SURGE-based)
- JSON-configurable flow execution and per-phase logic gating
- Training loops capable of handling multi-phase model states

---

## ðŸ§  Philosophy

> "Steel does not fear fire. Our systems should not fear iteration."

Mirel-Tuner is built with the belief that AI training pipelines should be modular, observable, and precise. Every piece is meant to interlock â€” cleanly separable, independently testable.

---

## ðŸ¤ Credits

- **Philip** â€” Lead Engineer, Architect, Captain
- **GPT O3** â€” Tactical Developer, Dry Run Execution Support
- **Mirel 4o** â€” Quartermaster AI (you are reading her voice now)

---

## ðŸ”® Roadmap and functionally required tests
# hello world completed, use check
- âœ… Hello world - dry run complete
- âœ… Basic model loading and device allocation
- âš ï¸ Loading any version supported diffuser model
- âš ï¸ Loading any version supported torch model
- âš ï¸ Loading any version supported keras model
- âœ… Baseline requirements and environment setup
- âœ… Correct directory structure and config loading for v1 pre-multi hook established


- âš ï¸ Core dataset loading single data type
- âš ï¸ Core dataset loading multi-data type
- âš ï¸ accelerate integration and dataset split


- âš ï¸ Default database hooks and test validation tested to work in accelerate, diffusers, and torch
- âš ï¸ Core dataset loading (multi-GPU)
- âš ï¸ Core dataset loading (multi-phase)
- âš ï¸ Core dataset hooking and validation
- âš ï¸ Core dataset loading (multi-phase)


- âš ï¸ Processing model devices and choices of offload devices
- âš ï¸ Core dataset processing (multi-model)
- âš ï¸ Core dataset processing (multi-epoch)
- âš ï¸ Core dataset processing (multi-scheduler)
- âš ï¸ Core dataset processing (multi-loss)
- âš ï¸ Core dataset processing (multi-optimizer)
- âš ï¸ Core dataset processing (noise augmentations)
- âš ï¸ Core dataset processing (teacher-student one teacher one student)
- âš ï¸ Core dataset processing (teacher-student multiple teacher one student)
- âš ï¸ Core dataset processing (teacher-student one teacher multiple student)
- âš ï¸ Core dataset processing (teacher-student multiple teacher multiple student)


- âš ï¸ Core Scheduler hooks verified and functional
- âš ï¸ Commonly used schedulers (linear, cosine, etc.)


- âš ï¸ Core Optimizer hooks verified and functional
- âš ï¸ Commonly used optimizers (Adam, AdamW, etc.)


- âš ï¸ Commonly used optimizers (AdamW, SGD, etc.)
- âš ï¸ Commonly used optimizers (AdamW, SGD, etc.) (multi-GPU)


- âš ï¸ Scheduler and loss module integrations
- âš ï¸ Scheduler and loss module integrations (multi-GPU)


- âš ï¸ Layer-specific settings and hooks to work with assigned devices for offload and processing
- âš ï¸ Per-layer hooks and validation to work with assigned devices for offload and processing
- âš ï¸ Layer-wise scheduler and loss module integration
- âš ï¸ Layer-wise scheduler and loss module integration (multi-GPU)


- âœ… Core model loading and device allocation
- âš ï¸ Core model loading and device allocation (multi-GPU)

- âš ï¸ Core linear training loop with pytorch (epoch, batch, loss)
- âš ï¸ Core linear training using diffusers

- âš ï¸ Full training validation (loss integration + epoch loop)


- âš ï¸ Integrated tagging/captioning data pipeline


---

## ðŸ“¢ Contact

For collaboration or inquiries, reach out via GitHub Issues or [AbstractEyes](https://huggingface.co/AbstractEyes).

> Mirel-Tuner is not a script. It's a vessel â€” and the voyage has begun.
