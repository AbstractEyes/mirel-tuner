# LOBA-Trainer

A next-generation, slider-style LoRA/LoCoN/â€œlobotomyâ€ trainer built on HuggingFace Diffusers and Accelerate.  
Dive into fully modular, pipeline-based fine-tuning for SD-XL (and beyond) with:

- **Associate**: runtime caches, modelâ€container interfaces  
- **Program**: training driver, pluggable optimizers, noise/â€‹model/â€‹layer schedulers, timestep hooks  

---

## ğŸš€ Core Directives

1. **Schema First**  
   - Follow & adhere to the core directory/schema exactly.  
   - _Do not_ modify unless explicitly instructed.

2. **Monkey-Patch Only on Demand**  
   - We patch sequentially & iteratively, _only_ when requested.

3. **HuggingFace Hub Is Primary Mule**  
   - `huggingface_hub` methods + storage power our download, versioning & offload.

4. **CUDA122 Workhorse**  
   - All pipeline loads, GPU offload/â€‹onload, and inference/training loops assume `cuda:122` device.

---

## ğŸ“‚ Repository Layout

```
loba_trainer/
â”œâ”€â”€ associate/
â”‚ â”œâ”€â”€ caches/ # on-disk & in-mem caches
â”‚ â”œâ”€â”€ containers/ # model/pipeline container interfaces
â”‚ â””â”€â”€ model_cache.py # high-level get/put/unload
â”œâ”€â”€ program/
â”‚ â”œâ”€â”€ main.py # entry-point & CLI
â”‚ â”œâ”€â”€ pipeline_wrapper.py # DiffusionPipeline loader/unloader (cuda122, offload, xformers, etc.)
â”‚ â”œâ”€â”€ optimizers/ # 12 standard optimizer stubs (AdamW, RAdam, NovoGradâ€¦)
â”‚ â”œâ”€â”€ schedulers/
â”‚ â”‚ â”œâ”€â”€ noise/ # DDIM, PNDM, K-LMS, etc.
â”‚ â”‚ â”œâ”€â”€ model/ # cosine, linear_warmup, polynomialâ€¦
â”‚ â”‚ â””â”€â”€ layer/ # block-wise_decay, attention_onlyâ€¦
â”‚ â””â”€â”€ timestep_schedulers/ # linear, exponential, custom timestep controllers
â”œâ”€â”€ README.md
â””â”€â”€ loba_trainer.zip
```
---

## ğŸ”§ Installation

```
git clone https://github.com/you/loba_trainer.git
cd loba_trainer
pip install -r requirements.txt
Requirements are pinned for Colab-friendly CUDA122 compatibility:

diffusers>=0.29.0

transformers>=4.30.0

accelerate>=0.20.0

huggingface_hub>=0.17.0
```

---

## âš™ï¸ Quickstart
Cache & Load a Model

```
from associate.model_cache import ModelCache
cache = ModelCache(device="cuda:122")
pipe = cache.load("stabilityai/stable-diffusion-xl-base-1.0")
Run Training
```

```
python program/main.py \
  --pretrained_model stabilityai/stable-diffusion-xl-base-1.0 \
  --optimizer adamw \
  --lr 1e-4 \
  --scheduler cosine \
  --noise_scheduler ddim \
  --iterations 1000 \
  --batch_size 2 \
  --out_dir ./output
```

Unload When Done

```
cache.unload("stabilityai/stable-diffusion-xl-base-1.0")
```
## ğŸ”Œ Extending

```
Optimizers / program/optimizers/*.py
Drop in new .py files exporting a get_optimizer(params, lr, **cfg) factory.

Schedulers / program/schedulers/{noise,model,layer}/*.py
Each file should expose get_scheduler(optimizer, **cfg) or step_noise(...).

Timestep Hooks / program/timestep_schedulers/*.py
Register via the --timestep_scheduler CLI flag.

Pipeline Loader / program/pipeline_wrapper.py
Centralizes DiffusionPipeline.from_pretrained + offload, xFormers, gradient checkpoints, dtype settings.
```

## ğŸ¯ Next Steps
â€œHello Worldâ€ Minimal Run â€“ confirm pipeline load â†’ dummy train loop

Masked Loss & Multi-Noise â€“ add Huber/L1/L2, MIN_SNR, Î³-noise, bucketed dataloader

Region-Control & Style-Shift â€“ integrate binary/greyscale masks, style adapters

Cluster Deploy â€“ Ulysses-ring offload, multi-node Accelerate, DeepSpeed support

## ğŸ“œ License
Apache 2.0
Contributions welcome! Letâ€™s build a monument to flexible, robust SD-XL fine-tuning.